{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPINNelX0c7IIR7uRJfKVxl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/josejailson/tensorflow_data_api/blob/main/tf_data_api.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NBCyuNRNkXMT"
      },
      "source": [
        "## Shuffling the Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zzBJXPYWh-4y",
        "outputId": "dfeaee13-10b9-4795-9455-79bbbaf7a67b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tf.Tensor([1 4 2 3 5 0 6], shape=(7,), dtype=int64)\n",
            "tf.Tensor([9 8 2 0 3 1 4], shape=(7,), dtype=int64)\n",
            "tf.Tensor([5 7 9 6 7 8], shape=(6,), dtype=int64)\n"
          ]
        }
      ],
      "source": [
        "dataset = tf.data.Dataset.range(10).repeat(2)\n",
        "dataset = dataset.shuffle(buffer_size=4, seed=42).batch(7)\n",
        "for item in dataset:\n",
        "    print(item)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_1HehoNhh-4y"
      },
      "source": [
        "### Interleaving lines from multiple files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZMSe0yVh-4y"
      },
      "source": [
        "Let's start by loading and preparing the California housing dataset. We first load it, then split it into a training set, a validation set and a test set:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O2FsPq9ih-4z"
      },
      "outputs": [],
      "source": [
        "# extra code – fetches, splits and normalizes the California housing dataset\n",
        "\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "housing = fetch_california_housing()\n",
        "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
        "    housing.data, housing.target.reshape(-1, 1), random_state=42)\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(\n",
        "    X_train_full, y_train_full, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mSoF0E3Lh-4z"
      },
      "source": [
        "For a very large dataset that does not fit in memory, you will typically want to split it into many files first, then have TensorFlow read these files in parallel. To demonstrate this, let's start by splitting the housing dataset and saving it to 20 CSV files:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8n0NRfn1h-40"
      },
      "outputs": [],
      "source": [
        "# extra code – split the dataset into 20 parts and save it to CSV files\n",
        "\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "def save_to_csv_files(data, name_prefix, header=None, n_parts=10):\n",
        "    housing_dir = Path() / \"datasets\" / \"housing\"\n",
        "    housing_dir.mkdir(parents=True, exist_ok=True)\n",
        "    filename_format = \"my_{}_{:02d}.csv\"\n",
        "\n",
        "    filepaths = []\n",
        "    m = len(data)\n",
        "    chunks = np.array_split(np.arange(m), n_parts)\n",
        "    for file_idx, row_indices in enumerate(chunks):\n",
        "        part_csv = housing_dir / filename_format.format(name_prefix, file_idx)\n",
        "        filepaths.append(str(part_csv))\n",
        "        with open(part_csv, \"w\") as f:\n",
        "            if header is not None:\n",
        "                f.write(header)\n",
        "                f.write(\"\\n\")\n",
        "            for row_idx in row_indices:\n",
        "                f.write(\",\".join([repr(col) for col in data[row_idx]]))\n",
        "                f.write(\"\\n\")\n",
        "    return filepaths\n",
        "\n",
        "train_data = np.c_[X_train, y_train]\n",
        "valid_data = np.c_[X_valid, y_valid]\n",
        "test_data = np.c_[X_test, y_test]\n",
        "header_cols = housing.feature_names + [\"MedianHouseValue\"]\n",
        "header = \",\".join(header_cols)\n",
        "\n",
        "train_filepaths = save_to_csv_files(train_data, \"train\", header, n_parts=20)\n",
        "valid_filepaths = save_to_csv_files(valid_data, \"valid\", header, n_parts=10)\n",
        "test_filepaths = save_to_csv_files(test_data, \"test\", header, n_parts=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EryHm5Shh-40"
      },
      "source": [
        "Okay, now let's take a peek at the first few lines of one of these CSV files:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P1qdfJD5h-41",
        "outputId": "dc865a3f-42e0-45c2-ce52-6d38451235cc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MedInc,HouseAge,AveRooms,AveBedrms,Population,AveOccup,Latitude,Longitude,MedianHouseValue\n",
            "3.5214,15.0,3.0499445061043287,1.106548279689234,1447.0,1.6059933407325193,37.63,-122.43,1.442\n",
            "5.3275,5.0,6.490059642147117,0.9910536779324056,3464.0,3.4433399602385686,33.69,-117.39,1.687\n",
            "3.1,29.0,7.5423728813559325,1.5915254237288134,1328.0,2.2508474576271187,38.44,-122.98,1.621\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"\".join(open(train_filepaths[0]).readlines()[:4]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ClzFcANFh-41",
        "outputId": "26afcafd-ea13-4b6e-9693-ed3638654238"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['datasets/housing/my_train_00.csv',\n",
              " 'datasets/housing/my_train_01.csv',\n",
              " 'datasets/housing/my_train_02.csv',\n",
              " 'datasets/housing/my_train_03.csv',\n",
              " 'datasets/housing/my_train_04.csv',\n",
              " 'datasets/housing/my_train_05.csv',\n",
              " 'datasets/housing/my_train_06.csv',\n",
              " 'datasets/housing/my_train_07.csv',\n",
              " 'datasets/housing/my_train_08.csv',\n",
              " 'datasets/housing/my_train_09.csv',\n",
              " 'datasets/housing/my_train_10.csv',\n",
              " 'datasets/housing/my_train_11.csv',\n",
              " 'datasets/housing/my_train_12.csv',\n",
              " 'datasets/housing/my_train_13.csv',\n",
              " 'datasets/housing/my_train_14.csv',\n",
              " 'datasets/housing/my_train_15.csv',\n",
              " 'datasets/housing/my_train_16.csv',\n",
              " 'datasets/housing/my_train_17.csv',\n",
              " 'datasets/housing/my_train_18.csv',\n",
              " 'datasets/housing/my_train_19.csv']"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_filepaths"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N1YTc5CJh-41"
      },
      "source": [
        "**Building an Input Pipeline**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X6ZOoGFBh-41"
      },
      "outputs": [],
      "source": [
        "filepath_dataset = tf.data.Dataset.list_files(train_filepaths, seed=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MqwAPqHZh-42",
        "outputId": "7101cb64-441a-426e-affc-3f22ff4bccc6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tf.Tensor(b'datasets/housing/my_train_05.csv', shape=(), dtype=string)\n",
            "tf.Tensor(b'datasets/housing/my_train_16.csv', shape=(), dtype=string)\n",
            "tf.Tensor(b'datasets/housing/my_train_01.csv', shape=(), dtype=string)\n",
            "tf.Tensor(b'datasets/housing/my_train_17.csv', shape=(), dtype=string)\n",
            "tf.Tensor(b'datasets/housing/my_train_00.csv', shape=(), dtype=string)\n",
            "tf.Tensor(b'datasets/housing/my_train_14.csv', shape=(), dtype=string)\n",
            "tf.Tensor(b'datasets/housing/my_train_10.csv', shape=(), dtype=string)\n",
            "tf.Tensor(b'datasets/housing/my_train_02.csv', shape=(), dtype=string)\n",
            "tf.Tensor(b'datasets/housing/my_train_12.csv', shape=(), dtype=string)\n",
            "tf.Tensor(b'datasets/housing/my_train_19.csv', shape=(), dtype=string)\n",
            "tf.Tensor(b'datasets/housing/my_train_07.csv', shape=(), dtype=string)\n",
            "tf.Tensor(b'datasets/housing/my_train_09.csv', shape=(), dtype=string)\n",
            "tf.Tensor(b'datasets/housing/my_train_13.csv', shape=(), dtype=string)\n",
            "tf.Tensor(b'datasets/housing/my_train_15.csv', shape=(), dtype=string)\n",
            "tf.Tensor(b'datasets/housing/my_train_11.csv', shape=(), dtype=string)\n",
            "tf.Tensor(b'datasets/housing/my_train_18.csv', shape=(), dtype=string)\n",
            "tf.Tensor(b'datasets/housing/my_train_04.csv', shape=(), dtype=string)\n",
            "tf.Tensor(b'datasets/housing/my_train_06.csv', shape=(), dtype=string)\n",
            "tf.Tensor(b'datasets/housing/my_train_03.csv', shape=(), dtype=string)\n",
            "tf.Tensor(b'datasets/housing/my_train_08.csv', shape=(), dtype=string)\n"
          ]
        }
      ],
      "source": [
        "# extra code – shows that the file paths are shuffled\n",
        "for filepath in filepath_dataset:\n",
        "    print(filepath)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JYXAvgAIh-42"
      },
      "outputs": [],
      "source": [
        "n_readers = 5\n",
        "dataset = filepath_dataset.interleave(\n",
        "    lambda filepath: tf.data.TextLineDataset(filepath).skip(1),\n",
        "    cycle_length=n_readers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qv0L5rRth-43",
        "outputId": "4a037889-2f30-4c72-8753-710223643591"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tf.Tensor(b'4.5909,16.0,5.475877192982456,1.0964912280701755,1357.0,2.9758771929824563,33.63,-117.71,2.418', shape=(), dtype=string)\n",
            "tf.Tensor(b'2.4792,24.0,3.4547038327526134,1.1341463414634145,2251.0,3.921602787456446,34.18,-118.38,2.0', shape=(), dtype=string)\n",
            "tf.Tensor(b'4.2708,45.0,5.121387283236994,0.953757225433526,492.0,2.8439306358381504,37.48,-122.19,2.67', shape=(), dtype=string)\n",
            "tf.Tensor(b'2.1856,41.0,3.7189873417721517,1.0658227848101265,803.0,2.0329113924050635,32.76,-117.12,1.205', shape=(), dtype=string)\n",
            "tf.Tensor(b'4.1812,52.0,5.701388888888889,0.9965277777777778,692.0,2.4027777777777777,33.73,-118.31,3.215', shape=(), dtype=string)\n"
          ]
        }
      ],
      "source": [
        "for line in dataset.take(5):\n",
        "    print(line)"
      ]
    }
  ]
}